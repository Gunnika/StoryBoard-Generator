{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stories = {}\n",
    "# source_folder = \"./clean_dataset\"\n",
    "\n",
    "# dir_paths = []\n",
    "# original_stories = []\n",
    "# story_names = []\n",
    "\n",
    "# for dir_name in os.listdir(source_folder):\n",
    "\n",
    "#     if not dir_name.startswith(\".DS_Store\"):\n",
    "#         dir_path = os.path.join(source_folder, dir_name)\n",
    "#         dir_paths.append(dir_path)\n",
    "#         for dir_path in dir_paths:\n",
    "#             for story_name in os.listdir(dir_path):\n",
    "#                 if not story_name.startswith(\".DS_Store\"):\n",
    "#                     story_names.append(story_name)\n",
    "#                     story_path = os.path.join(dir_path, story_name)\n",
    "#                     file_path = story_path + \"/story.txt\"\n",
    "#                     with open(file_path, 'r') as file:\n",
    "#                         file_content = file.read()\n",
    "#                     original_stories.append(file_content)\n",
    "\n",
    "# stories[\"original_story\"] = original_stories\n",
    "# stories[\"story_name\"] = story_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import sys\n",
    "import tqdm\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='2'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
    "from nltk.tokenize import TextTilingTokenizer\n",
    "nltk.download('punkt') \n",
    "nltk.download('stopwords')\n",
    "from transformers import pipeline\n",
    "from collections import defaultdict\n",
    "ttt = TextTilingTokenizer(w = 10, k=5)\n",
    "device = \"cuda:2\"\n",
    "\n",
    "def clean_segments(segments):\n",
    "    clean_segment = []\n",
    "    for segment in segments:\n",
    "        if len(segment.strip())>10:\n",
    "            clean_segment.append(segment)\n",
    "    return clean_segment\n",
    "\n",
    "\n",
    "def generateSegments(df):\n",
    "    print(\"Cleaning segments\")\n",
    "    works = 0\n",
    "    not_works = 0\n",
    "    all_segments = []\n",
    "\n",
    "    for index, row in tqdm.tqdm(df.iterrows()):\n",
    "    \n",
    "        story = row[\"original_story\"]\n",
    "        try:\n",
    "            segments = ttt.tokenize(story)\n",
    "            all_segments.append(clean_segments(segments))\n",
    "            works = works+1                \n",
    "            \n",
    "        except:\n",
    "            not_works = not_works + 1\n",
    "            all_segments.append([])\n",
    "            \n",
    "    print(\"segmentation works\", works)\n",
    "    print(\"segmentation doesn't work\", not_works)\n",
    "    \n",
    "    df[\"segmented_story\"] = all_segments\n",
    "    return df\n",
    "\n",
    "\n",
    "def generateSummaries(df, summarizer_model_id):\n",
    "    \n",
    "    pipe = pipeline(\"summarization\", model=summarizer_model_id, max_length=20, device = device)\n",
    "    all_summaries = []\n",
    "    \n",
    "    for index, row in tqdm.tqdm(df.iterrows()):\n",
    "        summaries = []\n",
    "        segments = row[\"segmented_story\"]\n",
    "        for segment in segments:\n",
    "            try:\n",
    "                summary = pipe(segment)\n",
    "                summaries.append(summary[0]['summary_text'])\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred with model {summarizer_model_id}: {e}\")\n",
    "                summaries.append(segment)\n",
    "                #append original segment in case of error\n",
    "        all_summaries.append(summaries)\n",
    "        \n",
    "    df[\"summarized_story\"] = all_summaries\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e6382ced62449a9dc2a2f826745b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646d0d221bd343128b99ab801500bdf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning segments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77it [00:25,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segmentation works 64\n",
      "segmentation doesn't work 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from utils import read_data\n",
    "from image_generation import generate_images\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='2'\n",
    "import nltk\n",
    "nltk.download('punkt')  # For tokenization\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import TextTilingTokenizer\n",
    "\n",
    "data_folder = \"../data/mini_dataset_storygen\"\n",
    "images_out_dir = \"../generated_images/baseline2\"\n",
    "df = read_data(data_folder)\n",
    "\n",
    "df = generateSegments(df)\n",
    "\n",
    "summarizer_model_id = \"sshleifer/distilbart-xsum-12-6\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/trunk/shared/cuneiform/CSE_291_GenAI/src/main.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bbaobab.ucsd.edu/trunk/shared/cuneiform/CSE_291_GenAI/src/main.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m df \u001b[39m=\u001b[39m generateSummaries(df, summarizer_model_id)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbaobab.ucsd.edu/trunk/shared/cuneiform/CSE_291_GenAI/src/main.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m df\u001b[39m.\u001b[39mto_csv(images_out_dir\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/df_summary.csv\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/trunk/shared/cuneiform/CSE_291_GenAI/src/main.ipynb Cell 5\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbaobab.ucsd.edu/trunk/shared/cuneiform/CSE_291_GenAI/src/main.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerateSummaries\u001b[39m(df, summarizer_model_id):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bbaobab.ucsd.edu/trunk/shared/cuneiform/CSE_291_GenAI/src/main.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m     pipe \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39m\u001b[39msummarization\u001b[39m\u001b[39m\"\u001b[39m, model\u001b[39m=\u001b[39msummarizer_model_id, max_length\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m, device \u001b[39m=\u001b[39m device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbaobab.ucsd.edu/trunk/shared/cuneiform/CSE_291_GenAI/src/main.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m     all_summaries \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbaobab.ucsd.edu/trunk/shared/cuneiform/CSE_291_GenAI/src/main.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m     \u001b[39mfor\u001b[39;00m index, row \u001b[39min\u001b[39;00m tqdm\u001b[39m.\u001b[39mtqdm(df\u001b[39m.\u001b[39miterrows()):\n",
      "File \u001b[0;32m~/miniconda3/envs/genai/lib/python3.11/site-packages/transformers/pipelines/__init__.py:1070\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1067\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1068\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m device\n\u001b[0;32m-> 1070\u001b[0m \u001b[39mreturn\u001b[39;00m pipeline_class(model\u001b[39m=\u001b[39mmodel, framework\u001b[39m=\u001b[39mframework, task\u001b[39m=\u001b[39mtask, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/genai/lib/python3.11/site-packages/transformers/pipelines/text2text_generation.py:67\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 67\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     69\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_model_type(\n\u001b[1;32m     70\u001b[0m         TF_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES\n\u001b[1;32m     71\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m         \u001b[39melse\u001b[39;00m MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES\n\u001b[1;32m     73\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/genai/lib/python3.11/site-packages/transformers/pipelines/base.py:797\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[0;34m(self, model, tokenizer, feature_extractor, image_processor, modelcard, framework, task, args_parser, device, torch_dtype, binary_output, **kwargs)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39m# We shouldn't call `model.to()` for models loaded with accelerate\u001b[39;00m\n\u001b[1;32m    796\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m device \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(device, \u001b[39mint\u001b[39m) \u001b[39mand\u001b[39;00m device \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[0;32m--> 797\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    799\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    800\u001b[0m     \u001b[39mif\u001b[39;00m hf_device_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    801\u001b[0m         \u001b[39m# Take the first device used by `accelerate`.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/genai/lib/python3.11/site-packages/transformers/modeling_utils.py:2271\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m     \u001b[39mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2267\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2268\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2269\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2270\u001b[0m         )\n\u001b[0;32m-> 2271\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mto(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/genai/lib/python3.11/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply(convert)\n",
      "File \u001b[0;32m~/miniconda3/envs/genai/lib/python3.11/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/genai/lib/python3.11/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/genai/lib/python3.11/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/envs/genai/lib/python3.11/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "df = generateSummaries(df, summarizer_model_id)\n",
    "\n",
    "df.to_csv(images_out_dir+\"/df_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate segment wise data for baseline\n",
    "# fairy_count = 0\n",
    "# story_count = 0\n",
    "# destination_folder = \"../data/mini_dataset_baseline_segment_wise/\"\n",
    "\n",
    "\n",
    "# for index,row in df.iterrows():\n",
    "#     if row[\"story_type\"] == \"fairy_tale\" and fairy_count<25:\n",
    "#         if len(row[\"segmented_story\"]) > 0:\n",
    "#             segments = row[\"segmented_story\"]\n",
    "#             fairy_count+=1\n",
    "#             for idx, segment in enumerate(segments):\n",
    "#                 file_name = row[\"story_name\"] + \"_segment_\" + str(idx)\n",
    "#                 with open(destination_folder+row[\"story_type\"]+\"/\"+file_name+\".txt\", \"w\") as file:\n",
    "#                     file.write(segment)\n",
    "        \n",
    "#     elif row[\"story_type\"] == \"short_story\" and story_count<25:\n",
    "#         if len(row[\"segmented_story\"]) > 0:\n",
    "#             segments = row[\"segmented_story\"]\n",
    "#             story_count+=1\n",
    "#             for idx, segment in enumerate(segments):\n",
    "#                 file_name = row[\"story_name\"] + \"_segment_\" + str(idx)\n",
    "#                 with open(destination_folder+row[\"story_type\"]+\"/\"+file_name+\".txt\", \"w\") as file:\n",
    "#                     file.write(segment)\n",
    "#     else:\n",
    "#         continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairy_tales_folder = \"../evaluation_data/baseline1/experiment_samit_fairy_tale_segmented\"\n",
    "short_story_folder = \"../evaluation_data/baseline1/experiment_samit_short_story_segmented\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
